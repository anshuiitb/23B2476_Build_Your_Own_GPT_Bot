# -*- coding: utf-8 -*-
"""Sentiment_Analysis_Using_Transformer_Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QOTzUt4Kz4kzAcX0IOIn7eA5LQW4Xwbe

Importing the Dependencies
"""

import os
import json

from zipfile import ZipFile
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""Data Collection - Kaggle API"""

kaggle_dictionary = json.load(open('kaggle.json'))

# setup kaggle credentials as environment variables
os.environ['KAGGLE_USERNAME'] = kaggle_dictionary['username']
os.environ['KAGGLE_KEY'] = kaggle_dictionary['key']

!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

!ls

# unzip the dataset file
with ZipFile('imdb-dataset-of-50k-movie-reviews.zip', 'r') as zip_ref:
  zip_ref.extractall()

!ls # to see the extracted file

"""Loading the Dataset"""

data = pd.read_csv('IMDB Dataset.csv')
df_sampled = data.sample(n=5000, random_state=42).reset_index(drop=True)

df_sampled.shape

df_sampled.head()

df_sampled.tail() # for printing the last five rows

df_sampled['sentiment'].value_counts()

df_sampled.replace({"sentiment": {"positive": 1, "negative": 0}}, inplace = True)

df_sampled.head()

# split data into training data and test data
train_data, test_data = train_test_split(df_sampled, test_size = 0.2, random_state = 42)

print(train_data.shape)
print(test_data.shape)

"""Data Pre-Processing"""

y_train = train_data['sentiment']
y_test = test_data['sentiment']

"""LSTM - Long Short Term Memory"""

from transformers import BertTokenizer, BertModel
import torch
import numpy as np
from tqdm import tqdm

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert = BertModel.from_pretrained("bert-base-uncased")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert.to(device)
bert.eval()

batch_size = 128
max_len = 200
texts = list(train_data['review'])
X_embed = []

for i in tqdm(range(0, len(texts), batch_size), desc="Embedding Batches"):
    batch_texts = texts[i:i+batch_size]
    inputs = tokenizer(batch_texts, return_tensors="pt", padding='max_length', truncation=True, max_length=max_len)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = bert(**inputs)
    batch_embeddings = outputs.last_hidden_state.cpu().numpy()  # shape: (batch_size, max_len, 768)
    X_embed.extend(batch_embeddings)

X_embed = np.array(X_embed)

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense

model = Sequential()
model.add(LSTM(128, input_shape=(200, 768), dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

# trainig the model
model.fit(X_embed, y_train, epochs = 5, batch_size = 64, validation_split = 0.2)

"""Model Evaluation"""

import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from tqdm import tqdm

# Load model and tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert = TFBertModel.from_pretrained("bert-base-uncased")
bert.trainable = False  # freeze BERT

# Ensure you use GPU if available
device = tf.device("/GPU:0" if tf.config.list_physical_devices("GPU") else "/CPU:0")

max_len = 200
test_texts = list(test_data['review'])
batch_size = 32

# Your test_texts is the list of sentences
x_test = []

with device:
    for i in tqdm(range(0, len(test_texts), batch_size), desc="Generating x_test"):
        batch_texts = test_texts[i:i + batch_size]

        # Tokenize batch
        inputs = tokenizer(
            batch_texts,
            padding='max_length',
            truncation=True,
            max_length=max_len,
            return_tensors='tf'
        )

        # Run BERT and extract embeddings
        outputs = bert(inputs['input_ids'], attention_mask=inputs['attention_mask'], training=False)
        last_hidden_state = outputs.last_hidden_state.numpy()  # (batch_size, 200, 768)

        x_test.append(last_hidden_state)

# Concatenate all batches to form final x_test
x_test = np.concatenate(x_test, axis=0)

print("x_test shape:", x_test.shape)
print("x_test dtype:", type(x_test))
print("y_test shape:", y_test.shape)
print("y_test dtype:", type(y_test))

print(type(x_test))         # Should be <class 'numpy.ndarray'>
print(np.array(x_test).shape)  # Should be (num_samples, 200, 768)

loss, accuracy = model.evaluate(x_test, y_test, batch_size=32)
print(f"Test loss: {loss:.4f}")
print(f"Test accuracy: {accuracy:.4f}")

"""Building Predictive System"""

import numpy as np
import tensorflow as tf

def predict_sentiment(review):
    # Tokenize and encode the review using BERT tokenizer
    encoded = tokenizer(review,
                        return_tensors='tf',
                        padding='max_length',
                        truncation=True,
                        max_length=200)

    # Ensure tensors are placed on CPU
    with tf.device('/CPU:0'):
        outputs = bert_model(encoded['input_ids'], attention_mask=encoded['attention_mask'])
        embedding = outputs.last_hidden_state  # shape: (1, 200, 768)

        # Predict sentiment using the LSTM model
        prediction = model.predict(embedding)

    sentiment = 'positive' if prediction[0][0] > 0.5 else 'negative'
    return sentiment

# example usage

new_review = "The story was really inspiring for budding sportsmen who belong to middle class families."
sentiment = predict_sentiment(new_review)
print(f"The sentiment of the review is: {sentiment}")

