{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7673b4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful. CSV saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the News Category Dataset v3 (newline-delimited JSON)\n",
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)\n",
    "\n",
    "# Save it as CSV\n",
    "df.to_csv('News_Category_Dataset_v3.csv', index=False)\n",
    "\n",
    "print(\"Conversion successful. CSV saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a1c02e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.iloc[:,1].values\n",
    "x=data.tolist()\n",
    "\n",
    "\n",
    "\n",
    "#x\n",
    "#x[150302]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "14ccca93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209527"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "63b97871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209521"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert to Series\n",
    "x_series = pd.Series(x)\n",
    "\n",
    "# Drop None and NaN\n",
    "x_cleaned = x_series.dropna()\n",
    "\n",
    "# Drop empty and whitespace-only strings\n",
    "x_cleaned = x_cleaned[~x_cleaned.astype(str).str.strip().eq(\"\")]\n",
    "\n",
    "# Update the original list\n",
    "x = x_cleaned.tolist()\n",
    "\n",
    "            \n",
    "len(x)  # (updated size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e7fbe2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "# filtered_words=[]\n",
    "# for i in range(len(x_tokenized)):\n",
    "#     for j in range(len(x_tokenized[i])):\n",
    "#         words=x_tokenized[i][j]\n",
    "#         filtered_words.append([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "# filtered_words\n",
    "\n",
    "sentences=x\n",
    "\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=word_tokenize(sentences[i])\n",
    "    words=[word.lower() for word in words if word.lower() not in stop_words]\n",
    "    sentences[i]=' '.join(words) # converting all the words into sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f164821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "228e5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "    sentences[i] = ' '.join(lemmatized_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f586d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "abcffd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d19bca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bcff1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sentence_vector(sentence, wv):\n",
    "    words = sentence.split()\n",
    "    # Filter out words not in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in wv]\n",
    "    if not valid_words:\n",
    "        # If no valid words, return zero vector\n",
    "        return np.zeros(wv.vector_size)\n",
    "    # Average word vectors\n",
    "    return np.mean(wv[valid_words], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b674395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = [sentence_vector(sent, wv) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "38d214ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if norm(vec1) == 0 or norm(vec2) == 0:\n",
    "        return 0\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b85ff8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_top5_similar(query, sentences, sentence_vectors, wv):\n",
    "#     query_vec = sentence_vector(query, wv)\n",
    "#     similarities = [cosine_similarity(query_vec, vec) for vec in sentence_vectors]\n",
    "#     # Get indices of top 5 scores\n",
    "#     top5_idx = np.argsort(similarities)[-5:][::-1]\n",
    "#     return [(sentences[i], similarities[i]) for i in top5_idx]\n",
    "\n",
    "def find_top5_similar(query, sentences, sentence_vectors, wv):\n",
    "    query_vec = sentence_vector(query, wv)\n",
    "    similarities = [cosine_similarity(query_vec, vec) for vec in sentence_vectors]\n",
    "    # Get indices of top 5 scores\n",
    "    top5_idx = np.argsort(similarities)[-5:][::-1]\n",
    "    return [(i, sentences[i], similarities[i]) for i in top5_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dd2a91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7130 | Index: 10045 | Sentence: guy 's run president want give 'free ' money\n",
      "0.6888 | Index: 138279 | Sentence: money nothing\n",
      "0.6834 | Index: 189577 | Sentence: get need little money\n",
      "0.6436 | Index: 43264 | Sentence: really sure want president penny ?\n",
      "0.6283 | Index: 150302 | Sentence: much money enough ?\n"
     ]
    }
   ],
   "source": [
    "# query = \"President got no money\"\n",
    "# results = find_top5_similar(query, sentences, sentence_vectors, wv)\n",
    "# for sent, score in results:\n",
    "#     print(f\"{score:.4f} => {sent}\")\n",
    "\n",
    "query = \"President got no money\"\n",
    "results = find_top5_similar(query, sentences, sentence_vectors, wv)\n",
    "\n",
    "for idx, sent, score in results:\n",
    "    print(f\"{score:.4f} | Index: {idx} | Sentence: {sent}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "79e6249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'much money enough ?'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[150302]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
